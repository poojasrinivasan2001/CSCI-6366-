{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing required libraries\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.onnx\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Select GPU to use otherwise CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "yvaCyCyXOGbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "WpuMEQ4tQiQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "14WELi2YQBSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading CSV file\n",
        "captions = pd.read_csv(\"/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/captions.csv\")\n",
        "captions"
      ],
      "metadata": {
        "id": "RosPg_v2OVG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displays a random image\n",
        "def display_random_data(count=5, seed=1):\n",
        "    np.random.seed(seed)\n",
        "    # random choose images == count\n",
        "    images = np.random.choice(captions['image'].unique(), count)\n",
        "    # display and their captions\n",
        "    for image in images:\n",
        "        # display image\n",
        "        display(Image.open(f'/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/images/{image}'))\n",
        "        # display caption\n",
        "        img_captions = captions.loc[captions['image']==image, 'caption'].tolist()\n",
        "        for cap in img_captions:\n",
        "            print(cap)\n",
        "display_random_data(2)"
      ],
      "metadata": {
        "id": "BAN0w2L1QSoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and preprocess dataset\n",
        "class My_Flickr1k(Dataset):\n",
        "    def __init__(self, root_file, captions, transform=None):\n",
        "\n",
        "        self.transform = transform\n",
        "        self.root = root_file\n",
        "        self.ids = captions\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_path, caption = self.ids[idx]\n",
        "        image = Image.open(self.root+image_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, caption\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ids.shape[0]"
      ],
      "metadata": {
        "id": "i49kA43MQdIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split captions and images into training and test data\n",
        "def build_datasets_vocab(root_file, captions_file, transform, split=0.15):\n",
        "    df = pd.read_csv(captions_file)\n",
        "\n",
        "    vocab = {}\n",
        "    def create_vocab(caption):\n",
        "        tokens = [token.lower() for token in word_tokenize(caption)]\n",
        "        for token in tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = len(vocab)\n",
        "\n",
        "    df[\"caption\"].apply(create_vocab)\n",
        "\n",
        "    train, valid = train_test_split(df, test_size=split, random_state=42)\n",
        "    return My_Flickr1k(root_file, train.values, transform), \\\n",
        "           My_Flickr1k(root_file, valid.values, transform), \\\n",
        "           vocab"
      ],
      "metadata": {
        "id": "mKio_7BRQfgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_dataset, valid_dataset, vocab = build_datasets_vocab(\"/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/images/\",\n",
        "                                              \"/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/captions.csv\",\n",
        "                                              transform)\n",
        "\n",
        "id_to_word = {id_: word for word, id_ in vocab.items()}"
      ],
      "metadata": {
        "id": "KKB2wy6RQkOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/captions.csv\")\n",
        "MAX_CAPTION_LEN = 38"
      ],
      "metadata": {
        "id": "x0bjdYbhQ6Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_captions(captions):\n",
        "\n",
        "    transformed = [[vocab[word.lower()] for word in word_tokenize(caption)] for caption in captions]\n",
        "    padded = [transform + [vocab[\".\"]]*(MAX_CAPTION_LEN - len(transform)) for transform in transformed]\n",
        "\n",
        "    return padded\n",
        "\n",
        "def get_caption(caption_sequence):\n",
        "\n",
        "    return \" \".join([id_to_word[id_] for id_ in caption_sequence if id_ != vocab[\".\"]])"
      ],
      "metadata": {
        "id": "MyOz2X2QQ_Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "POOLING_FACTOR = 32"
      ],
      "metadata": {
        "id": "B7on6cT5RBSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defines convolutional and transpose convolutional layers with LeakyReLU activation\n",
        "class ConvLeak(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
        "\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                      kernel_size=kernel_size, padding=(kernel_size-1)//2),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class ConvTransposeLeak(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                               kernel_size=kernel_size, padding=(kernel_size-1)//2),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)"
      ],
      "metadata": {
        "id": "IQRtuIUlRDKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defines a VAE encoder that extracts image features through convolution and pooling, then projects them into a latent space\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, image_dim, latent_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # constants used\n",
        "\n",
        "        iW, iH = image_dim\n",
        "        hW, hH = iW//POOLING_FACTOR, iH//POOLING_FACTOR\n",
        "        vec_dim = out_channels * hW * hH\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ConvLeak(in_channels=in_channels, out_channels=48),\n",
        "            ConvLeak(in_channels=48, out_channels=48)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ConvLeak(in_channels=48, out_channels=84),\n",
        "            ConvLeak(in_channels=84, out_channels=84)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ConvLeak(in_channels=84, out_channels=128),\n",
        "            ConvLeak(in_channels=128, out_channels=128)\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ConvLeak(in_channels=128, out_channels=out_channels),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.pooling = nn.MaxPool2d(4, return_indices=True)\n",
        "        self.pooling_2 = nn.MaxPool2d(2, return_indices=True)\n",
        "\n",
        "\n",
        "        self.hidden = nn.Sequential(\n",
        "            nn.Linear(in_features = vec_dim, out_features=latent_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(in_features=latent_dim, out_features=latent_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.encoder_mean = nn.Linear(in_features = latent_dim, out_features = vec_dim)\n",
        "        self.encoder_logstd = nn.Linear(in_features = latent_dim, out_features = vec_dim)\n",
        "\n",
        "\n",
        "    def generate_code(self, mean, log_std):\n",
        "\n",
        "        sigma = torch.exp(log_std)\n",
        "        epsilon = torch.randn_like(mean)\n",
        "        return (sigma * epsilon) + mean\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x, indices_1 = self.pooling(x)\n",
        "        x = self.layer2(x)\n",
        "        x, indices_2 = self.pooling(x)\n",
        "        x = self.layer3(x)\n",
        "        x, indices_3 = self.pooling_2(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        hidden = self.hidden(x)\n",
        "        mean, log_std = self.encoder_mean(hidden), self.encoder_logstd(hidden)\n",
        "        c = self.generate_code(mean, log_std)\n",
        "\n",
        "        return c, indices_1, indices_2, indices_3, mean, log_std"
      ],
      "metadata": {
        "id": "MY5koEzHRFa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defines a VAE decoder that reconstructs images from latent vectors\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, image_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        iW, iH = image_dim\n",
        "        hW, hH = iW//POOLING_FACTOR, iH//POOLING_FACTOR\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Unflatten(1, unflattened_size=(in_channels, hW, hH)),\n",
        "            ConvTransposeLeak(in_channels=in_channels, out_channels=128)\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ConvTransposeLeak(128, 128),\n",
        "            ConvTransposeLeak(128, 84)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ConvTransposeLeak(84, 84),\n",
        "            ConvTransposeLeak(84, 48)\n",
        "        )\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ConvTransposeLeak(48, 48),\n",
        "            ConvTransposeLeak(48, 3)\n",
        "        )\n",
        "\n",
        "        self.unpooling = nn.MaxUnpool2d(4)\n",
        "        self.unpooling_2 = nn.MaxUnpool2d(2)\n",
        "\n",
        "        self.precision = nn.Parameter(torch.rand(1))\n",
        "\n",
        "\n",
        "    def generate_data(self, mean, precision):\n",
        "\n",
        "\n",
        "        sigma = torch.exp(-precision)\n",
        "        epsilon = torch.randn_like(mean)\n",
        "        return (sigma * epsilon) + mean\n",
        "\n",
        "    def forward(self, x, indices_1, indices_2, indices_3):\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.unpooling_2(x, indices_3)\n",
        "        x = self.layer3(x)\n",
        "        x = self.unpooling(x, indices_2)\n",
        "        x = self.layer2(x)\n",
        "        x = self.unpooling(x, indices_1)\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tI0xNMXyRIr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Captioning model using GRU and cross attention over VAE-encoded image features\n",
        "class CaptionRNN(nn.Module):\n",
        "\n",
        "    CAPTION_LIMIT = MAX_CAPTION_LEN\n",
        "\n",
        "    def __init__(self, input_size, vocab_size, embedding_size, hidden_size, stop_index):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.mlp_l1 = nn.Sequential(\n",
        "            nn.Linear(in_features=input_size, out_features=input_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(in_features=input_size, out_features=hidden_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.mlp_l2 = nn.Sequential(\n",
        "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(in_features=hidden_size, out_features=vocab_size),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, batch_first=True)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "        self.stop_index = stop_index\n",
        "\n",
        "\n",
        "    def generate_caption(self, code):\n",
        "\n",
        "\n",
        "        h_1 = self.mlp_l1(code)\n",
        "        prob_1 = F.softmax(self.mlp_l2(h_1), dim=-1)\n",
        "        y_1 = torch.multinomial(prob_1, 1)\n",
        "\n",
        "        words = [y_1.item()]\n",
        "        w_t = self.embedding(y_1)\n",
        "        y_t = y_1\n",
        "        h_t = h_1\n",
        "\n",
        "\n",
        "        while len(words) < CaptionRNN.CAPTION_LIMIT and y_t.item() != self.stop_index:\n",
        "            h_t = self.gru(w_t.unsqueeze(0), h_t.unsqueeze(0).unsqueeze(0))[0]\n",
        "            h_t = h_t.squeeze(0).squeeze(0)\n",
        "            prob_t = F.softmax(self.mlp_l2(h_t), dim=-1)\n",
        "            y_t = torch.multinomial(prob_t, 1)\n",
        "            words.append(y_t.item())\n",
        "            w_t = self.embedding(y_t)\n",
        "\n",
        "        return words\n",
        "\n",
        "\n",
        "    def caption_prob(self, code, caption):\n",
        "\n",
        "        hidden_1 = self.mlp_l1(code)\n",
        "        probs_1 = F.softmax(self.mlp_l2(hidden_1), dim=1)\n",
        "        weights = self.embedding(caption)\n",
        "        output, hidden = self.gru(weights, hidden_1.unsqueeze(0))\n",
        "        probs_2_above = F.softmax(self.mlp_l2(output[:, :-1]), dim=-1)\n",
        "        return torch.cat([probs_1.unsqueeze(1), probs_2_above], dim=1)"
      ],
      "metadata": {
        "id": "c4ZtoJ_vRLq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combines VAE and captioning modules to reconstruct images and generate captions\n",
        "class VAECaptioner(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, code_channels, image_dim, vocab):\n",
        "        super().__init__()\n",
        "\n",
        "        LATENT_DIM = 300\n",
        "        EMBEDDING_SIZE = 600\n",
        "        HIDDEN_SIZE = 512\n",
        "        CODE_FLAT = code_channels*((image_dim[0]*image_dim[1])//(POOLING_FACTOR**2))\n",
        "\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.encoder = Encoder(in_channel, code_channels, image_dim, LATENT_DIM)\n",
        "        self.decoder = Decoder(code_channels, in_channel, image_dim)\n",
        "        self.captionr = CaptionRNN(CODE_FLAT, len(vocab), EMBEDDING_SIZE, HIDDEN_SIZE, vocab[\".\"])\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        c, indices_1, indices_2, indices_3, mean, log_std = self.encoder(x)\n",
        "        reconstructed = self.decoder(c, indices_1, indices_2, indices_3)\n",
        "        caption_prob = self.captionr.caption_prob(c, y)\n",
        "\n",
        "        return reconstructed, caption_prob, mean, log_std\n",
        "\n",
        "    def generate_caption(self, x):\n",
        "\n",
        "        c, indices_1, indices_2, indices_3, mean, log_std = self.encoder(x)\n",
        "        return self.captionr.generate_caption(c[0])"
      ],
      "metadata": {
        "id": "rv8B38PRRRJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializes training parameters, data loaders, model, optimizer, and loss functions for image reconstruction and caption generation\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "model = VAECaptioner(3, 128, (128, 128), vocab).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0002)\n",
        "criterion = nn.MSELoss(reduction=\"sum\")\n",
        "criterion2 = nn.CrossEntropyLoss(reduction=\"sum\")"
      ],
      "metadata": {
        "id": "csfj8cYtRTq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculates the total VAE loss\n",
        "def calculate_loss(reconstructed, caption_prob, images, captions_transformed, mean, log_std):\n",
        "\n",
        "    size = captions_transformed.shape[0]\n",
        "    reconstruction_error = criterion(reconstructed, images)\n",
        "    likelihoods = torch.stack([\n",
        "        caption_prob[i, np.arange(MAX_CAPTION_LEN), captions_transformed[i]] for i in range(size)])\n",
        "\n",
        "\n",
        "    log_likelihoods = -torch.log(likelihoods).sum()\n",
        "    KL_divergence = - (1 - mean.pow(2) - torch.exp(2 * log_std) + (2 *log_std)).sum()\n",
        "\n",
        "    return reconstruction_error + (log_likelihoods) + KL_divergence, log_likelihoods"
      ],
      "metadata": {
        "id": "MccP1C7iRYAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model while tracking losses\n",
        "losses = []\n",
        "caption_losses = []\n",
        "val_losses = []\n",
        "val_caption_losses = []\n",
        "for epoch in range(EPOCHS):\n",
        "    t = tqdm(train_dataloader, desc=f\"Train: Epoch {epoch}\")\n",
        "\n",
        "    for images, captions in t:\n",
        "        images = images.to(device)\n",
        "        captions_transformed = torch.LongTensor(transform_captions(captions)).to(device)\n",
        "        reconstructed, caption_prob, mean, log_std = model(images, captions_transformed)\n",
        "\n",
        "        loss, caption_loss = calculate_loss(reconstructed, caption_prob, images, captions_transformed, mean, log_std)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        losses.append(loss.item())\n",
        "        caption_losses.append(caption_loss.item())\n",
        "\n",
        "    v = tqdm(valid_dataloader, desc=f\"Valid: Epoch {epoch}\")\n",
        "    with torch.no_grad():\n",
        "        for images, captions in v:\n",
        "            images = images.to(device)\n",
        "            captions_transformed = torch.LongTensor(transform_captions(captions)).to(device)\n",
        "            reconstructed, caption_prob, mean, log_std = model(images, captions_transformed)\n",
        "            loss, caption_loss = calculate_loss(reconstructed, caption_prob, images, captions_transformed, mean, log_std)\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "            val_caption_losses.append(caption_loss.item())"
      ],
      "metadata": {
        "id": "hoCwFzjRRax6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'VAECaptioner.onnx')\n",
        "from torch.autograd import Variable\n",
        "trained_model = VAECaptioner(3, 128, (128, 128), vocab)\n",
        "trained_model.load_state_dict(torch.load('VAECaptioner.onnx'))\n",
        "dummy_input = Variable(torch.randn(1, 1, 28, 28))\n",
        "#torch.onnx.export(trained_model, dummy_input, \"VAECaptioner.onnx\")"
      ],
      "metadata": {
        "id": "bijtMy9sSRZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_model = VAECaptioner(3, 128, (128, 128), vocab).to(device)\n",
        "model_path = \"VAECaptioner.onnx\"\n",
        "\n",
        "# Initialize model with the weights\n",
        "map_location = lambda storage, loc: storage\n",
        "if torch.cuda.is_available():\n",
        "    map_location = None\n",
        "torch_model.load_state_dict(torch.load(model_path, map_location=map_location))"
      ],
      "metadata": {
        "id": "P7jzNmE3STr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "24pogsLxSWIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(caption_losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time - For Captions')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aLLRAlagSY-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.imshow(images[4].to(\"cpu\").permute(1, 2, 0))\n",
        "plt.axis(\"off\")\n",
        "_ = plt.title(get_caption(model.generate_caption(images[4].unsqueeze(0))))"
      ],
      "metadata": {
        "id": "a0Lr1-PtSgn9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}