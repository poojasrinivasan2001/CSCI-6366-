{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing required libraries\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.onnx\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Select GPU to use otherwise CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "yvaCyCyXOGbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "WpuMEQ4tQiQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "14WELi2YQBSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading CSV file\n",
        "captions = pd.read_csv(\"/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/captions.csv\")\n",
        "captions"
      ],
      "metadata": {
        "id": "RosPg_v2OVG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displays a random image\n",
        "def display_random_data(count=5, seed=1):\n",
        "    np.random.seed(seed)\n",
        "    # random choose images == count\n",
        "    images = np.random.choice(captions['image'].unique(), count)\n",
        "    # display and their captions\n",
        "    for image in images:\n",
        "        # display image\n",
        "        display(Image.open(f'/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/images/{image}'))\n",
        "        # display caption\n",
        "        img_captions = captions.loc[captions['image']==image, 'caption'].tolist()\n",
        "        for cap in img_captions:\n",
        "            print(cap)\n",
        "display_random_data(2)"
      ],
      "metadata": {
        "id": "BAN0w2L1QSoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and preprocess dataset\n",
        "class My_Flickr1k(Dataset):\n",
        "    def __init__(self, root_file, captions, transform=None):\n",
        "\n",
        "        self.transform = transform\n",
        "        self.root = root_file\n",
        "        self.ids = captions\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_path, caption = self.ids[idx]\n",
        "        image = Image.open(self.root+image_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, caption\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ids.shape[0]"
      ],
      "metadata": {
        "id": "i49kA43MQdIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split captions and images into training and test data\n",
        "def build_datasets_vocab(root_file, captions_file, transform, split=0.15):\n",
        "    df = pd.read_csv(captions_file)\n",
        "\n",
        "    vocab = {}\n",
        "    def create_vocab(caption):\n",
        "        tokens = [token.lower() for token in word_tokenize(caption)]\n",
        "        for token in tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = len(vocab)\n",
        "\n",
        "    df[\"caption\"].apply(create_vocab)\n",
        "\n",
        "    train, valid = train_test_split(df, test_size=split, random_state=42)\n",
        "    return My_Flickr1k(root_file, train.values, transform), \\\n",
        "           My_Flickr1k(root_file, valid.values, transform), \\\n",
        "           vocab"
      ],
      "metadata": {
        "id": "mKio_7BRQfgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_dataset, valid_dataset, vocab = build_datasets_vocab(\"/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/images/\",\n",
        "                                              \"/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/captions.csv\",\n",
        "                                              transform)\n",
        "\n",
        "id_to_word = {id_: word for word, id_ in vocab.items()}"
      ],
      "metadata": {
        "id": "KKB2wy6RQkOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NN_BASEMODEL/flicke 1k/flickr1k/captions.csv\")\n",
        "# MAX_CAPTION_LEN = df[\"caption\"].apply(lambda x: len(word_tokenize(x))).max()\n",
        "MAX_CAPTION_LEN = 38"
      ],
      "metadata": {
        "id": "x0bjdYbhQ6Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms text captions into padded token ID sequences and decodes token IDs back into readable captions.\n",
        "def transform_captions(captions):\n",
        "\n",
        "    transformed = [[vocab[word.lower()] for word in word_tokenize(caption)] for caption in captions]\n",
        "    padded = [transform + [vocab[\".\"]]*(MAX_CAPTION_LEN - len(transform)) for transform in transformed]\n",
        "\n",
        "    return padded\n",
        "\n",
        "def get_caption(caption_sequence):\n",
        "\n",
        "    return \" \".join([id_to_word[id_] for id_ in caption_sequence if id_ != vocab[\".\"]])"
      ],
      "metadata": {
        "id": "MyOz2X2QQ_Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "POOLING_FACTOR = 32"
      ],
      "metadata": {
        "id": "B7on6cT5RBSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defines convolutional and transpose convolutional layers with LeakyReLU activation\n",
        "class ConvLeak(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
        "\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                      kernel_size=kernel_size, padding=(kernel_size-1)//2),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class ConvTransposeLeak(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                               kernel_size=kernel_size, padding=(kernel_size-1)//2),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)"
      ],
      "metadata": {
        "id": "IQRtuIUlRDKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defines a VAE encoder that extracts image features through convolution and pooling, then projects them into a latent space\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, image_dim, latent_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # constants used\n",
        "\n",
        "        iW, iH = image_dim\n",
        "        hW, hH = iW//POOLING_FACTOR, iH//POOLING_FACTOR\n",
        "        vec_dim = out_channels * hW * hH\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ConvLeak(in_channels=in_channels, out_channels=48),\n",
        "            ConvLeak(in_channels=48, out_channels=48)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ConvLeak(in_channels=48, out_channels=84),\n",
        "            ConvLeak(in_channels=84, out_channels=84)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ConvLeak(in_channels=84, out_channels=128),\n",
        "            ConvLeak(in_channels=128, out_channels=128)\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ConvLeak(in_channels=128, out_channels=out_channels),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.pooling = nn.MaxPool2d(4, return_indices=True)\n",
        "        self.pooling_2 = nn.MaxPool2d(2, return_indices=True)\n",
        "\n",
        "\n",
        "        self.hidden = nn.Sequential(\n",
        "            nn.Linear(in_features = vec_dim, out_features=latent_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(in_features=latent_dim, out_features=latent_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.encoder_mean = nn.Linear(in_features = latent_dim, out_features = vec_dim)\n",
        "        self.encoder_logstd = nn.Linear(in_features = latent_dim, out_features = vec_dim)\n",
        "\n",
        "\n",
        "    def generate_code(self, mean, log_std):\n",
        "\n",
        "        sigma = torch.exp(log_std)\n",
        "        epsilon = torch.randn_like(mean)\n",
        "        return (sigma * epsilon) + mean\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x, indices_1 = self.pooling(x)\n",
        "        x = self.layer2(x)\n",
        "        x, indices_2 = self.pooling(x)\n",
        "        x = self.layer3(x)\n",
        "        x, indices_3 = self.pooling_2(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        hidden = self.hidden(x)\n",
        "        mean, log_std = self.encoder_mean(hidden), self.encoder_logstd(hidden)\n",
        "        c = self.generate_code(mean, log_std)\n",
        "\n",
        "        return c, indices_1, indices_2, indices_3, mean, log_std"
      ],
      "metadata": {
        "id": "MY5koEzHRFa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defines a VAE decoder that reconstructs images from latent vectors\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, image_dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        iW, iH = image_dim\n",
        "        hW, hH = iW//POOLING_FACTOR, iH//POOLING_FACTOR\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Unflatten(1, unflattened_size=(in_channels, hW, hH)),\n",
        "            ConvTransposeLeak(in_channels=in_channels, out_channels=128)\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ConvTransposeLeak(128, 128),\n",
        "            ConvTransposeLeak(128, 84)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ConvTransposeLeak(84, 84),\n",
        "            ConvTransposeLeak(84, 48)\n",
        "        )\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ConvTransposeLeak(48, 48),\n",
        "            ConvTransposeLeak(48, 3)\n",
        "        )\n",
        "\n",
        "        self.unpooling = nn.MaxUnpool2d(4)\n",
        "        self.unpooling_2 = nn.MaxUnpool2d(2)\n",
        "\n",
        "        self.precision = nn.Parameter(torch.rand(1))\n",
        "\n",
        "\n",
        "    def generate_data(self, mean, precision):\n",
        "\n",
        "\n",
        "        sigma = torch.exp(-precision)\n",
        "        epsilon = torch.randn_like(mean)\n",
        "        return (sigma * epsilon) + mean\n",
        "\n",
        "    def forward(self, x, indices_1, indices_2, indices_3):\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.unpooling_2(x, indices_3)\n",
        "        x = self.layer3(x)\n",
        "        x = self.unpooling(x, indices_2)\n",
        "        x = self.layer2(x)\n",
        "        x = self.unpooling(x, indices_1)\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tI0xNMXyRIr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p3oktM1QfcFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing Cross Attention\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(query_dim, hidden_dim)\n",
        "        self.key_proj = nn.Linear(context_dim, hidden_dim)\n",
        "        self.value_proj = nn.Linear(context_dim, hidden_dim)\n",
        "        self.out_proj = nn.Linear(hidden_dim, context_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, query, context):\n",
        "\n",
        "        query = self.query_proj(query).unsqueeze(1)  # (batch, 1, hidden_dim)\n",
        "        key = self.key_proj(context)                 # (batch, seq_len, hidden_dim)\n",
        "        value = self.value_proj(context)             # (batch, seq_len, hidden_dim)\n",
        "\n",
        "        scores = torch.bmm(query, key.transpose(1, 2))  # (batch, 1, seq_len)\n",
        "        attn_weights = self.softmax(scores)             # (batch, 1, seq_len)\n",
        "\n",
        "        attended = torch.bmm(attn_weights, value)        # (batch, 1, hidden_dim)\n",
        "        attended = attended.squeeze(1)                   # (batch, hidden_dim)\n",
        "\n",
        "        output = self.out_proj(attended)                 # (batch, context_dim)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "avQke6D3OSzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Captioning model using GRU and cross attention over VAE-encoded image features\n",
        "class CaptionRNN(nn.Module):\n",
        "    CAPTION_LIMIT = MAX_CAPTION_LEN\n",
        "\n",
        "    def __init__(self, input_size, vocab_size, embedding_size, hidden_size, stop_index):\n",
        "        super().__init__()\n",
        "\n",
        "        self.code_seq_len = 64  # because 128x128 images downsampled\n",
        "        self.code_dim = input_size // self.code_seq_len\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size + self.code_dim, hidden_size, batch_first=True)\n",
        "\n",
        "        # Cross Attention\n",
        "        self.cross_attention = CrossAttention(hidden_size, context_dim=self.code_dim, hidden_dim=hidden_size)\n",
        "        self.context_proj = nn.Linear(self.code_dim, embedding_size)\n",
        "\n",
        "        # Output MLP\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(hidden_size, vocab_size)\n",
        "        )\n",
        "\n",
        "        self.init_hidden_proj = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.stop_index = stop_index\n",
        "\n",
        "    def generate_caption(self, code):\n",
        "        batch_size = code.size(0)\n",
        "        code_seq = code.view(batch_size, self.code_seq_len, self.code_dim) # Reshape latent vector to sequence for attention\n",
        "\n",
        "        h_t = self.init_hidden_proj(code)\n",
        "\n",
        "        context = self.cross_attention(h_t, code_seq)\n",
        "\n",
        "\n",
        "        first_logits = self.fc_out(h_t)\n",
        "        y_t = torch.multinomial(F.softmax(first_logits, dim=-1), 1)\n",
        "        w_t = self.embedding(y_t)\n",
        "\n",
        "        words = [y_t.item()]\n",
        "\n",
        "        for _ in range(CaptionRNN.CAPTION_LIMIT - 1):\n",
        "            if words[-1] == self.stop_index and len(words) >= 5:  # Stop if EOS token appears after a few words\n",
        "                break\n",
        "\n",
        "            gru_input = torch.cat((w_t, context.unsqueeze(1)), dim=-1)  # (batch, 1, embedding+code_dim)\n",
        "            out, h_t_new = self.gru(gru_input, h_t.unsqueeze(0))\n",
        "            h_t = out.squeeze(1)\n",
        "\n",
        "            context = self.cross_attention(h_t, code_seq)\n",
        "\n",
        "\n",
        "            logits = self.fc_out(h_t)\n",
        "            y_t = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
        "\n",
        "            words.append(y_t.item())\n",
        "            w_t = self.embedding(y_t)\n",
        "\n",
        "        return words\n",
        "\n",
        "\n",
        "\n",
        "    def caption_prob(self, code, caption):\n",
        "        batch_size = code.size(0)\n",
        "        code_seq = code.view(batch_size, self.code_seq_len, self.code_dim)\n",
        "\n",
        "        h_t = self.init_hidden_proj(code)\n",
        "        caption_embed = self.embedding(caption) # Embed input caption\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(caption.size(1)):\n",
        "            context = self.cross_attention(h_t, code_seq)\n",
        "\n",
        "\n",
        "            gru_input = torch.cat((caption_embed[:, t:t+1], context.unsqueeze(1)), dim=-1)\n",
        "            out, h_t = self.gru(gru_input, h_t.unsqueeze(0))\n",
        "            h_t = out.squeeze(1)\n",
        "\n",
        "            logits = self.fc_out(h_t)  # Predict token distribution\n",
        "            outputs.append(logits.unsqueeze(1)) # Store predictions\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        return outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "q5RSggl8OoHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combines VAE and captioning modules to reconstruct images and generate captions\n",
        "class VAECaptioner(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, code_channels, image_dim, vocab):\n",
        "        super().__init__()\n",
        "\n",
        "        LATENT_DIM = 300\n",
        "        EMBEDDING_SIZE = 600\n",
        "        HIDDEN_SIZE = 512\n",
        "        CODE_FLAT = code_channels*((image_dim[0]*image_dim[1])//(POOLING_FACTOR**2))\n",
        "\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.encoder = Encoder(in_channel, code_channels, image_dim, LATENT_DIM)\n",
        "        self.decoder = Decoder(code_channels, in_channel, image_dim)\n",
        "        self.captionr = CaptionRNN(CODE_FLAT, len(vocab), EMBEDDING_SIZE, HIDDEN_SIZE, vocab[\".\"])\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        c, indices_1, indices_2, indices_3, mean, log_std = self.encoder(x) # Encode image to latent representation\n",
        "        reconstructed = self.decoder(c, indices_1, indices_2, indices_3) # Reconstruct image from code\n",
        "        caption_prob = self.captionr.caption_prob(c, y) # Compute caption token probabilities\n",
        "\n",
        "        return reconstructed, caption_prob, mean, log_std\n",
        "\n",
        "    def generate_caption(self, x):\n",
        "\n",
        "        c, indices_1, indices_2, indices_3, mean, log_std = self.encoder(x)\n",
        "        return self.captionr.generate_caption(c[0])"
      ],
      "metadata": {
        "id": "rv8B38PRRRJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializes training parameters, data loaders, model, optimizer, and loss functions for image reconstruction and caption generation\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "model = VAECaptioner(3, 128, (128, 128), vocab).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0002)\n",
        "criterion = nn.MSELoss(reduction=\"sum\")\n",
        "criterion2 = nn.CrossEntropyLoss(reduction=\"sum\")"
      ],
      "metadata": {
        "id": "csfj8cYtRTq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculates the total VAE loss\n",
        "def calculate_loss(reconstructed, caption_prob, images, captions_transformed, mean, log_std, kl_weight):\n",
        "    reconstruction_error = criterion(reconstructed, images)\n",
        "\n",
        "    caption_prob = caption_prob.permute(0, 2, 1)\n",
        "    caption_loss = criterion2(caption_prob, captions_transformed)\n",
        "\n",
        "    KL_divergence = - (1 - mean.pow(2) - torch.exp(2 * log_std) + (2 * log_std)).sum()\n",
        "\n",
        "    total_loss = reconstruction_error + caption_loss + kl_weight * KL_divergence\n",
        "    return total_loss, caption_loss\n"
      ],
      "metadata": {
        "id": "MccP1C7iRYAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model while tracking losses\n",
        "losses = []\n",
        "caption_losses = []\n",
        "val_losses = []\n",
        "val_caption_losses = []\n",
        "\n",
        "kl_annealing_steps = 45  # based on batch size and 3 epochs\n",
        "current_step = 0         # counter for total updates\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    t = tqdm(train_dataloader, desc=f\"Train: Epoch {epoch}\")\n",
        "\n",
        "    for images, captions in t:\n",
        "        images = images.to(device)\n",
        "        captions_transformed = torch.LongTensor(transform_captions(captions)).to(device)\n",
        "\n",
        "        reconstructed, caption_prob, mean, log_std = model(images, captions_transformed)\n",
        "\n",
        "        kl_weight = min(1.0, current_step / kl_annealing_steps)  # KL annealing\n",
        "        loss, caption_loss = calculate_loss(reconstructed, caption_prob, images, captions_transformed, mean, log_std, kl_weight)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        caption_losses.append(caption_loss.item())\n",
        "        current_step += 1\n",
        "\n",
        "    v = tqdm(valid_dataloader, desc=f\"Valid: Epoch {epoch}\")\n",
        "    with torch.no_grad():\n",
        "        for images, captions in v:\n",
        "            images = images.to(device)\n",
        "            captions_transformed = torch.LongTensor(transform_captions(captions)).to(device)\n",
        "            reconstructed, caption_prob, mean, log_std = model(images, captions_transformed)\n",
        "\n",
        "\n",
        "            loss, caption_loss = calculate_loss(reconstructed, caption_prob, images, captions_transformed, mean, log_std, kl_weight=1.0)\n",
        "\n",
        "            val_losses.append(loss.item())\n",
        "            val_caption_losses.append(caption_loss.item())\n"
      ],
      "metadata": {
        "id": "hoCwFzjRRax6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'VAECaptioner.onnx')\n",
        "from torch.autograd import Variable\n",
        "trained_model = VAECaptioner(3, 128, (128, 128), vocab)\n",
        "trained_model.load_state_dict(torch.load('VAECaptioner.onnx'))\n",
        "dummy_input = Variable(torch.randn(1, 1, 28, 28))\n",
        "#torch.onnx.export(trained_model, dummy_input, \"VAECaptioner.onnx\")"
      ],
      "metadata": {
        "id": "bijtMy9sSRZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_model = VAECaptioner(3, 128, (128, 128), vocab).to(device)\n",
        "model_path = \"VAECaptioner.onnx\"\n",
        "\n",
        "# Initialize model with the weights\n",
        "map_location = lambda storage, loc: storage\n",
        "if torch.cuda.is_available():\n",
        "    map_location = None\n",
        "torch_model.load_state_dict(torch.load(model_path, map_location=map_location))"
      ],
      "metadata": {
        "id": "P7jzNmE3STr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w-nkXoOHSvnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(caption_losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time - For Captions')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aLLRAlagSY-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Select and prepare the image\n",
        "img = images[4].unsqueeze(0).to(device)  # (1, 3, 128, 128)\n",
        "\n",
        "# Step 2: Pass through encoder to get latent code\n",
        "code, _, _, _, _, _ = model.encoder(img)\n",
        "\n",
        "# Step 3: Pass code into generate_caption\n",
        "caption_ids = model.captionr.generate_caption(code)\n",
        "\n",
        "# Step 4: Show the image\n",
        "plt.imshow(images[4].cpu().permute(1, 2, 0))\n",
        "plt.axis(\"off\")\n",
        "_ = plt.title(get_caption(caption_ids))"
      ],
      "metadata": {
        "id": "Tszlz5GI8dci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Select and prepare the image\n",
        "img = images[4].unsqueeze(0).to(device)  # (1, 3, 128, 128)\n",
        "\n",
        "# Step 2: Pass through encoder to get latent code\n",
        "code, _, _, _, _, _ = model.encoder(img)\n",
        "\n",
        "# Step 3: Pass code into generate_caption\n",
        "caption_ids = model.captionr.generate_caption(code)\n",
        "\n",
        "# Step 4: Show the image\n",
        "plt.imshow(images[4].cpu().permute(1, 2, 0))\n",
        "plt.axis(\"off\")\n",
        "_ = plt.title(get_caption(caption_ids))\n"
      ],
      "metadata": {
        "id": "WLsPp47EWaPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Select and prepare the image\n",
        "img = images[4].unsqueeze(0).to(device)  # (1, 3, 128, 128)\n",
        "\n",
        "# Step 2: Pass through encoder to get latent code\n",
        "code, _, _, _, _, _ = model.encoder(img)  # encoder returns 6 things, we only care about first one\n",
        "\n",
        "# Step 3: Pass code into generate_caption (no second unsqueeze needed)\n",
        "caption_ids = model.captionr.generate_caption(code)  # code already has batch dimension\n",
        "\n",
        "# Step 4: Show the image\n",
        "plt.imshow(images[4].cpu().permute(1, 2, 0))\n",
        "plt.axis(\"off\")\n",
        "_ = plt.title(get_caption(caption_ids))"
      ],
      "metadata": {
        "id": "LwaR9SSu18JB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}